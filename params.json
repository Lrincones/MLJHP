{
  "name": "Mljhp",
  "tagline": "Repository for John Hopkins Practical Machine Learning Course",
  "body": "#---\r\ntitle: \"Practical Machine Learning\"\r\nauthor: \"Luis Rincones\"\r\ndate: \"6 June 2016\"\r\noutput: html_document\r\n---\r\n## Preparation  \r\nPrepare two data frames one for each file(training and testing)\r\n\r\nBackground for the variable classe\r\nIt corresponds to Unilateral Dumbbell Biceps Curl in five different fashions: according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. \r\nGenerate an Histogram to see the frequencies of the classe variable.\r\n\r\nEliminate the features with NA, from both sets(training and testing)\r\nEliminate some features that don't contribute to the model we are planning to create\r\n\r\n## Summary\r\nAfter reviewing the available information in the site for the \"Human Activity Recognition\", in particular the document available in\r\n\"http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf\".\r\nThe strategy was based in generating 4 subsets one for each sensor;\r\nARM, BELT, DUMBBELLL and FOREARM.\r\nThe goal was to seek the features that were common among all sensors with a significant contribution to their corresponding model. With this set of features, a new model will be created for prediction. Our requirements were to have a much smaller set of features than the original 159 (without classe). We would like to have fewer than 16 (an empiric number to make the computational times acceptable) \r\n\r\nFor each sensor a model was built the corresponding rfcv was used to assist in checking the model.\r\n\r\n## Libraries\r\n```{r libraries}\r\nlibrary(caret)\r\nlibrary(randomForest)\r\n```\r\n\r\n## Preparation\r\nAfter downloading the files and creating the Data Frames, generate an Histogram of the Variable classe for the training set.\r\n```{r preparation}\r\nset.seed(1947)\r\ndownload.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\", \"training.csv\")\r\ndownload.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\", \"testing.csv\")\r\ntraining = read.csv(\"training.csv\")\r\ntesting = read.csv(\"testing.csv\")\r\nbarplot(summary(training$classe))\r\ncount_na <-sapply(training, function(y) sum(length(which(is.na(y)))))\r\nlist_na <- which(count_na > 0) \r\ntraining_small <- training[,-list_na] \r\ntesting_small <- testing[,-list_na]\r\ntraining_small <- training_small[,-c(1,3:7)]\r\ntesting_small <- testing_small[,-c(1,3:7)]\r\nnombres <- colnames(training_small)\r\n```\r\n## Sensors Data Frame\r\nAs indicated before we will divide the training Data Frame in  4 sets with a smaller number of features each.\r\n```{r Sensors_Data_Sets}\r\n### Recall RandomForest \r\n### Can not handle categorical predictors with more than 53 categories.\r\n# Sensor ARM\r\ntemp <- grep(\"_arm\",nombres, ignore.case = TRUE)\r\ntemp <- append(temp, 87, after=length(temp))\r\nt_arm <- training_small[,temp]\r\ncount_factors <- sapply(t_arm, function(y) which(nlevels(y) <= 50))\r\ncount_factors <- as.logical(count_factors)\r\ncount_factors[is.na(count_factors)] <- FALSE\r\nt_arm_small <- t_arm[,count_factors] \r\nt_arm_modFit <- randomForest(classe ~ ., t_arm_small, importance=TRUE)\r\n# Sensor BELT\r\ntemp <- grep(\"_belt\",nombres, ignore.case = TRUE)\r\ntemp <- append(temp, 87, after=length(temp))\r\nt_belt <- training_small[,temp]\r\ncount_factors <- sapply(t_belt, function(y) which(nlevels(y) <= 50))\r\ncount_factors <- as.logical(count_factors)\r\ncount_factors[is.na(count_factors)] <- FALSE\r\nt_belt_small <- t_belt[,count_factors] \r\nt_belt_modFit <- randomForest(classe ~ ., t_belt_small, importance=TRUE)\r\n# SENSOR DUMBBELLL\r\ntemp <- grep(\"_dumbbell\",nombres, ignore.case = TRUE)\r\ntemp <- append(temp, 87, after=length(temp))\r\nt_dumbbell <- training_small[,temp]\r\ncount_factors <- sapply(t_dumbbell, function(y) which(nlevels(y) <= 50))\r\ncount_factors <- as.logical(count_factors)\r\ncount_factors[is.na(count_factors)] <- FALSE\r\nt_dumbbell_small <- t_dumbbell[,count_factors] \r\nt_dumbbell_modFit <- randomForest(classe ~ ., t_dumbbell_small, importance=TRUE)\r\n# SENSOR FOREARM\r\ntemp <- grep(\"_forearm\",nombres, ignore.case = TRUE)\r\ntemp <- append(temp, 87, after=length(temp))\r\nt_forearm <- training_small[,temp]\r\nt_forearm <- training_small[,temp]\r\ncount_factors <- sapply(t_forearm, function(y) which(nlevels(y) <= 50))\r\ncount_factors <- as.logical(count_factors)\r\ncount_factors[is.na(count_factors)] <- FALSE\r\nt_forearm_small <- t_forearm[,count_factors] \r\nt_forearm_modFit <- randomForest(classe ~ ., t_dumbbell_small, importance=TRUE)\r\n\r\n```\r\n## Analysis\r\nReview the features importance for each of the sensor's set.\r\nGenerate the Random Forest Cross-Validation for feature selection (rfcv)\r\n```{r Analyzing_Sensors_Model}\r\n# Listing and Analyzing the features for each sensor's model\r\nsort(t_arm_modFit$importance[,7], decreasing = TRUE)\r\nsort(t_belt_modFit$importance[,7], decreasing = TRUE)\r\nsort(t_dumbbell_modFit$importance[,7], decreasing = TRUE)\r\nsort(t_forearm_modFit$importance[,7], decreasing = TRUE)\r\ncolnames(t_arm_small)\r\ncolnames(t_belt_small)\r\ncolnames(t_dumbbell_small)\r\ncolnames(t_forearm_small)\r\nt_arm_rfcv <- rfcv(trainx=t_arm_small[,-14], trainy = t_arm_small[,14], scale = \"log\", step=0.5)\r\nt_belt_rfcv <- rfcv(trainx=t_belt_small[,-17], trainy = t_belt_small[,17], scale = \"log\", step=0.5)\r\nt_dumbbell_rfcv <- rfcv(trainx=t_dumbbell_small[,-17], trainy = t_dumbbell_small[,17], scale = \"log\", step=0.5)\r\nt_forearm_rfcv <- rfcv(trainx=t_forearm_small[,-19], trainy = t_forearm_small[,19], scale = \"log\", step=0.5)\r\n```\r\n## Final Model\r\nAfter the analysis, the features we found as significant and common were the \"magnet\" for each of the 3 dimensions(x,y,z) and from each of the four sensors. Those had the more consistent behavior with our goals\r\nWe now create a training set with only those features. \r\nIt is important to mention that for the belt sensor, the magnet group were the second set of features, the first 3 places were (yaw_belt, roll_belt and pitch_belt). \r\nFor lack of time, considering this as part of the Final Model, will be done in future versions.\r\n```{r Final Model}\r\n# after analysis the features chosen as a common group were\r\ntemp1 <- grep(\"magnet\",nombres, ignore.case = TRUE)\r\ntemp1 <- grep(\"magnet\",nombres, ignore.case = TRUE)\r\ntemp1 <- append(temp1, 87, after=length(temp1))\r\nt_final <- training_small[,temp1]\r\ndim(t_final)\r\ncolnames(t_final)\r\nt_final_modFit <- randomForest(classe ~ ., t_final, importance=TRUE)\r\nt_final_modFit$confusion\r\nt_final_modFit$importance\r\nt_rfcv <- rfcv(trainx=t_final[,-13], trainy = t_final[,13], scale = \"log\", step=0.7)\r\nt_rfcv$error.cv\r\n```\r\n## Prediction\r\nAs required the prediction using the testing as newdata.\r\nResults are indicate below.\r\n```{r Prediction }\r\nt_prediction <- predict(t_final_modFit, newdata = testing_small)\r\nt_prediction\r\n```\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}